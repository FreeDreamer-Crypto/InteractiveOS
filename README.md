# InteractiveOS

System for processing gestural data for analysis, biofeedback, and advanced forms of human/computer interaction.

3D skeletal motion, as captured by devices like Kinect include measured body joint positions, velocities, relative angles between body parts.  Gestures formed can be inferred from analysis of this data stream.

## Visualization
Draw the skeleton in 3D space, displaying measurements as optional layers.  Playback of recordings can be paused, time-stretched, and reversed.  Real-time live display is an additional mode that the interface can switch seamlessly between.

## Gesture Learning
"Gesture learning" is intended to have 2 simultaneous meanings:

### Software Learning Human Gestures
Software learning human gestures and their patterns and quantifiable spatiotemporal measurements.  These gestures can be stored in a shared database for all users to access for playback and comparison.  Generalizations of these gestures can be learned to distill the essence of what the gestures represent, and how they are similar and different to others.

### Humans Learning Gestures
Human learning of known gestures via the feedback and assistance of a software critique.  This involves comparing a user's movement to known sequences, offering useful guidence for the user to align with the recorded movement in various ways.

This includes ability to recognize an over-extension of a limb, the torso, or the neck, and to detect approxiate weight distribution to bring the users awareness to adjust accordingly (per-posture correction 
as opposed to sequence correction, possible because of weight/center detection).
 
Learning particular characteristics and features of an individual user's body and its capabilities
(taking in paramters such as mass distribution, age, injury, past medical issues...etc.) allows the system to discover the path of greatest aid for that person. 

Audio feedback, generated by sonifying gestural data in arbitrary ways, provides the user an additional sensory mode of awareness of kinesthetic state.  Subtle changes in body pose can be sensed by changes in the sound.  This may be helpful to people with vision and other impairments.  This applies to both pre-recorded and live usage modes.  Sonification of pre-recorded gestures can be useful in demonstrating all aspects of a gesture in multiple sensory modes.


## PureData Interface
Provide skeletal data to PureData (http://puredata.info).  PureData provides a way to rapidly inspect, visualize, and develop applications as a dataflow model.

----

# How to Help
 * develop the C# portion of this kit as a generic helper and starting point for more specialized projects

----

# Resources

## Kinect Interface
http://msdn.microsoft.com/en-us/library/microsoft.kinect.aspx


## PureData
http://puredata.info/ Download the extended version. 


## Sonification
C# OSC Implementation: http://opensoundcontrol.org/implementation/osc-net-v1-2
I'm using version 1.4.1, which the DLL is in the main directory.


## Spline Functions for Robot Movements
( from: https://groups.google.com/forum/#!topic/opencog/uIq5SgnDScg ) 
http://ais.informatik.uni-freiburg.de/publications/papers/lau09iros.pdf
http://www.cs.cmu.edu/~zkolter/pubs/kolter-icra09a.pdf

CMA-ES learning mechanism for learning the spline coefficients:
https://www.lri.fr/~hansen/cmaes_inmatlab.html


## Flexible Muscle-Based Locomotion for Bipedal Creatures
( from: https://groups.google.com/forum/#!topic/opencog/uIq5SgnDScg ) 
http://www.staff.science.uu.nl/~geijt101/papers/SA2013/SA2013.pdf
Uses Covariance Matrix Adaptation (CMA-ES, mentioned in your message) and a biologically inspired physical model (including neural delays and biomechanical constraints) to get very realistic and natural movement
